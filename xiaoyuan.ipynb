{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_image(number, size=(28, 14)):\n",
    "    # 定义数字到字符的映射\n",
    "    number_to_char = {\n",
    "        0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'\n",
    "    }\n",
    "\n",
    "    # 将数字转换为字符\n",
    "    char = number_to_char[number]\n",
    "\n",
    "    # 创建一个空白图片\n",
    "    img = Image.new('L', size[-1::-1], color=255)\n",
    "\n",
    "    # 在图片上绘制数字\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype('arial.ttf', 28)  # 你可以根据需要选择其他字体\n",
    "    draw.text((0, 0), char, font=font, fill=0)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberDataset:\n",
    "    def __init__(self, size=(28, 14), iters=1000):\n",
    "        self.iters = iters\n",
    "        self.images = [np.array(number_to_image(number, size)) for number in range(10)]\n",
    "\n",
    "    def __getitem__(self, _):\n",
    "        num = random.randint(0, 9)\n",
    "        return self.images[num][None,...], num\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rec(nn.Module):\n",
    "    def __init__(self, size=(28, 14)):\n",
    "        super(Rec, self).__init__()\n",
    "        self.row = nn.Parameter(torch.ones(1, 1, size[0]))\n",
    "        self.col = nn.Parameter(torch.ones(1, 1, size[1]))\n",
    "        self.weights = nn.Parameter(torch.ones(1, 1, size[0], size[1]))\n",
    "        self.classifier = nn.Linear(3, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Softmax for row and col\n",
    "        row_softmax = torch.softmax(self.row, dim=-1)\n",
    "        col_softmax = torch.softmax(self.col, dim=-1)\n",
    "        # Multiply row and col\n",
    "        multiplied_row_col = row_softmax.unsqueeze(-1) *  col_softmax.unsqueeze(-2)\n",
    "        \n",
    "        # Element-wise multiplication with weights and x\n",
    "        weighted_x = x * multiplied_row_col * self.weights\n",
    "        weighted_x = weighted_x.flatten(1)\n",
    "\n",
    "        h = []\n",
    "        for _ in range(3):\n",
    "            v, idx = torch.max(weighted_x, 1)\n",
    "            h.append(v)\n",
    "            idx = nn.functional.one_hot(idx, weighted_x.size(1))\n",
    "            weighted_x = weighted_x * (1 - idx)\n",
    "        \n",
    "        h = torch.stack(h, dim=1)\n",
    "        \n",
    "        return self.classifier(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_size=32, iter=1000, size=(28, 14)):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = NumberDataset(size, iter)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    model = Rec(size).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Iter {i}, Loss: {loss.item()}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Loss: 2.4886677265167236\n",
      "Iter 1, Loss: 2.2672061920166016\n",
      "Iter 2, Loss: 2.5261788368225098\n",
      "Iter 3, Loss: 2.4924709796905518\n",
      "Iter 4, Loss: 2.403188467025757\n",
      "Iter 5, Loss: 2.509498119354248\n",
      "Iter 6, Loss: 2.3198537826538086\n",
      "Iter 7, Loss: 2.283595561981201\n",
      "Iter 8, Loss: 2.4199378490448\n",
      "Iter 9, Loss: 2.275085210800171\n",
      "Iter 10, Loss: 2.458631753921509\n",
      "Iter 11, Loss: 2.294389486312866\n",
      "Iter 12, Loss: 2.469709873199463\n",
      "Iter 13, Loss: 2.4438679218292236\n",
      "Iter 14, Loss: 2.2742748260498047\n",
      "Iter 15, Loss: 2.350750207901001\n",
      "Iter 16, Loss: 2.2735185623168945\n",
      "Iter 17, Loss: 2.3753228187561035\n",
      "Iter 18, Loss: 2.312957286834717\n",
      "Iter 19, Loss: 2.289559841156006\n",
      "Iter 20, Loss: 2.4434070587158203\n",
      "Iter 21, Loss: 2.311736583709717\n",
      "Iter 22, Loss: 2.2251698970794678\n",
      "Iter 23, Loss: 2.215135335922241\n",
      "Iter 24, Loss: 2.2347946166992188\n",
      "Iter 25, Loss: 2.2686538696289062\n",
      "Iter 26, Loss: 2.290188789367676\n",
      "Iter 27, Loss: 2.3577957153320312\n",
      "Iter 28, Loss: 2.4085276126861572\n",
      "Iter 29, Loss: 2.2971243858337402\n",
      "Iter 30, Loss: 2.3618462085723877\n",
      "Iter 31, Loss: 2.352604389190674\n",
      "Iter 32, Loss: 2.1523993015289307\n",
      "Iter 33, Loss: 2.21177077293396\n",
      "Iter 34, Loss: 2.052877187728882\n",
      "Iter 35, Loss: 2.365144729614258\n",
      "Iter 36, Loss: 2.1183958053588867\n",
      "Iter 37, Loss: 2.247046709060669\n",
      "Iter 38, Loss: 2.2630133628845215\n",
      "Iter 39, Loss: 2.1331448554992676\n",
      "Iter 40, Loss: 2.058561086654663\n",
      "Iter 41, Loss: 2.1735646724700928\n",
      "Iter 42, Loss: 2.0553951263427734\n",
      "Iter 43, Loss: 2.0215139389038086\n",
      "Iter 44, Loss: 1.9167314767837524\n",
      "Iter 45, Loss: 1.9614384174346924\n",
      "Iter 46, Loss: 1.7518199682235718\n",
      "Iter 47, Loss: 1.9842315912246704\n",
      "Iter 48, Loss: 1.5771064758300781\n",
      "Iter 49, Loss: 1.6306272745132446\n",
      "Iter 50, Loss: 1.6652953624725342\n",
      "Iter 51, Loss: 1.514953851699829\n",
      "Iter 52, Loss: 1.8036056756973267\n",
      "Iter 53, Loss: 1.4588394165039062\n",
      "Iter 54, Loss: 1.3265011310577393\n",
      "Iter 55, Loss: 1.3039144277572632\n",
      "Iter 56, Loss: 1.2694242000579834\n",
      "Iter 57, Loss: 1.6076207160949707\n",
      "Iter 58, Loss: 1.4532004594802856\n",
      "Iter 59, Loss: 1.0790657997131348\n",
      "Iter 60, Loss: 1.5514987707138062\n",
      "Iter 61, Loss: 1.1085052490234375\n",
      "Iter 62, Loss: 1.27933931350708\n",
      "Iter 63, Loss: 1.663035273551941\n",
      "Iter 64, Loss: 1.0023317337036133\n",
      "Iter 65, Loss: 0.9898506999015808\n",
      "Iter 66, Loss: 1.4987304210662842\n",
      "Iter 67, Loss: 1.2218822240829468\n",
      "Iter 68, Loss: 1.0169522762298584\n",
      "Iter 69, Loss: 0.7567071318626404\n",
      "Iter 70, Loss: 1.1331056356430054\n",
      "Iter 71, Loss: 0.9797597527503967\n",
      "Iter 72, Loss: 1.1403143405914307\n",
      "Iter 73, Loss: 0.8002183437347412\n",
      "Iter 74, Loss: 0.9030942320823669\n",
      "Iter 75, Loss: 0.7475331425666809\n",
      "Iter 76, Loss: 1.204516053199768\n",
      "Iter 77, Loss: 1.3922092914581299\n",
      "Iter 78, Loss: 0.8168751001358032\n",
      "Iter 79, Loss: 0.8544197082519531\n",
      "Iter 80, Loss: 0.8251505494117737\n",
      "Iter 81, Loss: 1.2167037725448608\n",
      "Iter 82, Loss: 0.8738815188407898\n",
      "Iter 83, Loss: 0.6361814737319946\n",
      "Iter 84, Loss: 0.7614584565162659\n",
      "Iter 85, Loss: 1.0681402683258057\n",
      "Iter 86, Loss: 0.7475823163986206\n",
      "Iter 87, Loss: 0.6364513039588928\n",
      "Iter 88, Loss: 0.6646997332572937\n",
      "Iter 89, Loss: 0.8315062522888184\n",
      "Iter 90, Loss: 0.3821440637111664\n",
      "Iter 91, Loss: 0.5254663825035095\n",
      "Iter 92, Loss: 0.481594055891037\n",
      "Iter 93, Loss: 0.6594170928001404\n",
      "Iter 94, Loss: 0.5157480835914612\n",
      "Iter 95, Loss: 0.5223253965377808\n",
      "Iter 96, Loss: 0.3573445677757263\n",
      "Iter 97, Loss: 0.5408834218978882\n",
      "Iter 98, Loss: 0.5230226516723633\n",
      "Iter 99, Loss: 0.34572210907936096\n",
      "Iter 100, Loss: 0.39401403069496155\n",
      "Iter 101, Loss: 0.43581444025039673\n",
      "Iter 102, Loss: 0.3055788278579712\n",
      "Iter 103, Loss: 0.2915097773075104\n",
      "Iter 104, Loss: 0.547879159450531\n",
      "Iter 105, Loss: 0.23062923550605774\n",
      "Iter 106, Loss: 0.5127202868461609\n",
      "Iter 107, Loss: 0.5142793655395508\n",
      "Iter 108, Loss: 0.4564061164855957\n",
      "Iter 109, Loss: 0.41561374068260193\n",
      "Iter 110, Loss: 0.6466643810272217\n",
      "Iter 111, Loss: 0.72093665599823\n",
      "Iter 112, Loss: 0.9742013216018677\n",
      "Iter 113, Loss: 1.6257959604263306\n",
      "Iter 114, Loss: 0.8211613297462463\n",
      "Iter 115, Loss: 1.9166560173034668\n",
      "Iter 116, Loss: 1.8193726539611816\n",
      "Iter 117, Loss: 0.3464464247226715\n",
      "Iter 118, Loss: 1.081946611404419\n",
      "Iter 119, Loss: 1.4538533687591553\n",
      "Iter 120, Loss: 0.9613526463508606\n",
      "Iter 121, Loss: 1.2319660186767578\n",
      "Iter 122, Loss: 0.9869212508201599\n",
      "Iter 123, Loss: 1.6120874881744385\n",
      "Iter 124, Loss: 1.0691630840301514\n",
      "Iter 125, Loss: 1.0625677108764648\n",
      "Iter 126, Loss: 0.670609176158905\n",
      "Iter 127, Loss: 0.7031450867652893\n",
      "Iter 128, Loss: 0.5923417210578918\n",
      "Iter 129, Loss: 0.6942882537841797\n",
      "Iter 130, Loss: 0.4184216856956482\n",
      "Iter 131, Loss: 0.829276442527771\n",
      "Iter 132, Loss: 0.6694503426551819\n",
      "Iter 133, Loss: 0.585541844367981\n",
      "Iter 134, Loss: 0.5672705769538879\n",
      "Iter 135, Loss: 0.5612301826477051\n",
      "Iter 136, Loss: 0.45246750116348267\n",
      "Iter 137, Loss: 0.5039310455322266\n",
      "Iter 138, Loss: 0.49894019961357117\n",
      "Iter 139, Loss: 0.5198304653167725\n",
      "Iter 140, Loss: 0.35472503304481506\n",
      "Iter 141, Loss: 0.3868151903152466\n",
      "Iter 142, Loss: 0.2936519980430603\n",
      "Iter 143, Loss: 0.45585647225379944\n",
      "Iter 144, Loss: 0.35133078694343567\n",
      "Iter 145, Loss: 0.23925729095935822\n",
      "Iter 146, Loss: 0.26153770089149475\n",
      "Iter 147, Loss: 0.22934241592884064\n",
      "Iter 148, Loss: 0.2496679127216339\n",
      "Iter 149, Loss: 0.18915359675884247\n",
      "Iter 150, Loss: 0.26101261377334595\n",
      "Iter 151, Loss: 0.20192676782608032\n",
      "Iter 152, Loss: 0.18798553943634033\n",
      "Iter 153, Loss: 0.25191137194633484\n",
      "Iter 154, Loss: 0.12037809193134308\n",
      "Iter 155, Loss: 0.18457810580730438\n",
      "Iter 156, Loss: 0.20245680212974548\n",
      "Iter 157, Loss: 0.2569960355758667\n",
      "Iter 158, Loss: 0.10490848124027252\n",
      "Iter 159, Loss: 0.19359520077705383\n",
      "Iter 160, Loss: 0.1186220645904541\n",
      "Iter 161, Loss: 0.11612088233232498\n",
      "Iter 162, Loss: 0.13714461028575897\n",
      "Iter 163, Loss: 0.1404085010290146\n",
      "Iter 164, Loss: 0.08607175201177597\n",
      "Iter 165, Loss: 0.2017303854227066\n",
      "Iter 166, Loss: 0.18592830002307892\n",
      "Iter 167, Loss: 0.13451886177062988\n",
      "Iter 168, Loss: 0.10132209956645966\n",
      "Iter 169, Loss: 0.3680391311645508\n",
      "Iter 170, Loss: 0.16481754183769226\n",
      "Iter 171, Loss: 0.2781483232975006\n",
      "Iter 172, Loss: 0.5073509812355042\n",
      "Iter 173, Loss: 0.8135915398597717\n",
      "Iter 174, Loss: 0.270580917596817\n",
      "Iter 175, Loss: 0.23124337196350098\n",
      "Iter 176, Loss: 0.5648453831672668\n",
      "Iter 177, Loss: 0.13156920671463013\n",
      "Iter 178, Loss: 0.502217710018158\n",
      "Iter 179, Loss: 0.09597281366586685\n",
      "Iter 180, Loss: 0.1497424989938736\n",
      "Iter 181, Loss: 0.21519291400909424\n",
      "Iter 182, Loss: 0.1316305547952652\n",
      "Iter 183, Loss: 0.20586714148521423\n",
      "Iter 184, Loss: 0.21482156217098236\n",
      "Iter 185, Loss: 0.10259201377630234\n",
      "Iter 186, Loss: 0.3963395357131958\n",
      "Iter 187, Loss: 0.10832754522562027\n",
      "Iter 188, Loss: 0.11386911571025848\n",
      "Iter 189, Loss: 0.09361185878515244\n",
      "Iter 190, Loss: 0.053957171738147736\n",
      "Iter 191, Loss: 0.0828007385134697\n",
      "Iter 192, Loss: 0.09853442758321762\n",
      "Iter 193, Loss: 0.060512784868478775\n",
      "Iter 194, Loss: 0.07687211036682129\n",
      "Iter 195, Loss: 0.09755288064479828\n",
      "Iter 196, Loss: 0.032604340463876724\n",
      "Iter 197, Loss: 0.05616921931505203\n",
      "Iter 198, Loss: 0.06338020414113998\n",
      "Iter 199, Loss: 0.028107699006795883\n",
      "Iter 200, Loss: 0.030081909149885178\n",
      "Iter 201, Loss: 0.028657637536525726\n",
      "Iter 202, Loss: 0.051084067672491074\n",
      "Iter 203, Loss: 0.042077723890542984\n",
      "Iter 204, Loss: 0.020693672820925713\n",
      "Iter 205, Loss: 0.02908584661781788\n",
      "Iter 206, Loss: 0.02770700864493847\n",
      "Iter 207, Loss: 0.026622038334608078\n",
      "Iter 208, Loss: 0.02832082100212574\n",
      "Iter 209, Loss: 0.016226762905716896\n",
      "Iter 210, Loss: 0.019145570695400238\n",
      "Iter 211, Loss: 0.01673535257577896\n",
      "Iter 212, Loss: 0.02466265670955181\n",
      "Iter 213, Loss: 0.022696278989315033\n",
      "Iter 214, Loss: 0.043656397610902786\n",
      "Iter 215, Loss: 0.011498704552650452\n",
      "Iter 216, Loss: 0.013614985160529613\n",
      "Iter 217, Loss: 0.03653189539909363\n",
      "Iter 218, Loss: 0.027508236467838287\n",
      "Iter 219, Loss: 0.02520187944173813\n",
      "Iter 220, Loss: 0.02943439409136772\n",
      "Iter 221, Loss: 0.03388388454914093\n",
      "Iter 222, Loss: 0.012464724481105804\n",
      "Iter 223, Loss: 0.011412449181079865\n",
      "Iter 224, Loss: 0.015022297389805317\n",
      "Iter 225, Loss: 0.012413767166435719\n",
      "Iter 226, Loss: 0.012886612676084042\n",
      "Iter 227, Loss: 0.009970503859221935\n",
      "Iter 228, Loss: 0.011140773072838783\n",
      "Iter 229, Loss: 0.013297892175614834\n",
      "Iter 230, Loss: 0.015553216449916363\n",
      "Iter 231, Loss: 0.009677509777247906\n",
      "Iter 232, Loss: 0.012253276072442532\n",
      "Iter 233, Loss: 0.008555149659514427\n",
      "Iter 234, Loss: 0.009999475441873074\n",
      "Iter 235, Loss: 0.007764316163957119\n",
      "Iter 236, Loss: 0.004664183594286442\n",
      "Iter 237, Loss: 0.007554043550044298\n",
      "Iter 238, Loss: 0.004762151278555393\n",
      "Iter 239, Loss: 0.005563954822719097\n",
      "Iter 240, Loss: 0.005184308160096407\n",
      "Iter 241, Loss: 0.007755550555884838\n",
      "Iter 242, Loss: 0.003609868697822094\n",
      "Iter 243, Loss: 0.0034220044035464525\n",
      "Iter 244, Loss: 0.0036611980758607388\n",
      "Iter 245, Loss: 0.003537937067449093\n",
      "Iter 246, Loss: 0.0037822576705366373\n",
      "Iter 247, Loss: 0.004080874379724264\n",
      "Iter 248, Loss: 0.0030213624704629183\n",
      "Iter 249, Loss: 0.0028082020580768585\n",
      "Iter 250, Loss: 0.0030383921694010496\n",
      "Iter 251, Loss: 0.002778221620246768\n",
      "Iter 252, Loss: 0.002396221971139312\n",
      "Iter 253, Loss: 0.0026110000908374786\n",
      "Iter 254, Loss: 0.00261942227371037\n",
      "Iter 255, Loss: 0.00169308390468359\n",
      "Iter 256, Loss: 0.0035567404702305794\n",
      "Iter 257, Loss: 0.0023129486944526434\n",
      "Iter 258, Loss: 0.0029506951104849577\n",
      "Iter 259, Loss: 0.002260659821331501\n",
      "Iter 260, Loss: 0.0018177505116909742\n",
      "Iter 261, Loss: 0.0023387400433421135\n",
      "Iter 262, Loss: 0.002597132232040167\n",
      "Iter 263, Loss: 0.002572152763605118\n",
      "Iter 264, Loss: 0.002501059090718627\n",
      "Iter 265, Loss: 0.002278357045724988\n",
      "Iter 266, Loss: 0.0018060288857668638\n",
      "Iter 267, Loss: 0.0016570708248764277\n",
      "Iter 268, Loss: 0.002155766123905778\n",
      "Iter 269, Loss: 0.0013876643497496843\n",
      "Iter 270, Loss: 0.0017287470400333405\n",
      "Iter 271, Loss: 0.002356779295951128\n",
      "Iter 272, Loss: 0.0014316873857751489\n",
      "Iter 273, Loss: 0.002249684650450945\n",
      "Iter 274, Loss: 0.0017510732868686318\n",
      "Iter 275, Loss: 0.002188190119341016\n",
      "Iter 276, Loss: 0.0015612045535817742\n",
      "Iter 277, Loss: 0.0014270441606640816\n",
      "Iter 278, Loss: 0.001698275445960462\n",
      "Iter 279, Loss: 0.002063983818516135\n",
      "Iter 280, Loss: 0.0019995172042399645\n",
      "Iter 281, Loss: 0.0016814020927995443\n",
      "Iter 282, Loss: 0.0015368795720860362\n",
      "Iter 283, Loss: 0.0011641773162409663\n",
      "Iter 284, Loss: 0.0017896339995786548\n",
      "Iter 285, Loss: 0.0015908663626760244\n",
      "Iter 286, Loss: 0.0016430928371846676\n",
      "Iter 287, Loss: 0.0010925860842689872\n",
      "Iter 288, Loss: 0.0014148806221783161\n",
      "Iter 289, Loss: 0.0015695167239755392\n",
      "Iter 290, Loss: 0.00119449058547616\n",
      "Iter 291, Loss: 0.0018886030884459615\n",
      "Iter 292, Loss: 0.0014623997267335653\n",
      "Iter 293, Loss: 0.0014647679636254907\n",
      "Iter 294, Loss: 0.0014542755670845509\n",
      "Iter 295, Loss: 0.0017052085604518652\n",
      "Iter 296, Loss: 0.0016243597492575645\n",
      "Iter 297, Loss: 0.0016389129450544715\n",
      "Iter 298, Loss: 0.0015939668519422412\n",
      "Iter 299, Loss: 0.0014664878835901618\n",
      "Iter 300, Loss: 0.0012580621987581253\n",
      "Iter 301, Loss: 0.0015994614223018289\n",
      "Iter 302, Loss: 0.0011209752410650253\n",
      "Iter 303, Loss: 0.0010340705048292875\n",
      "Iter 304, Loss: 0.000999725190922618\n",
      "Iter 305, Loss: 0.0015342606930062175\n",
      "Iter 306, Loss: 0.0012199814664199948\n",
      "Iter 307, Loss: 0.0013555390760302544\n",
      "Iter 308, Loss: 0.0013569856528192759\n",
      "Iter 309, Loss: 0.0010137418285012245\n",
      "Iter 310, Loss: 0.0014396889600902796\n",
      "Iter 311, Loss: 0.0014107887400314212\n",
      "Iter 312, Loss: 0.0013478321488946676\n",
      "Iter 313, Loss: 0.0011041631223633885\n",
      "Iter 314, Loss: 0.001103299087844789\n",
      "Iter 315, Loss: 0.0012865825556218624\n",
      "Iter 316, Loss: 0.0010950560681521893\n",
      "Iter 317, Loss: 0.0016474006697535515\n",
      "Iter 318, Loss: 0.001274301903322339\n",
      "Iter 319, Loss: 0.0011639847652986646\n",
      "Iter 320, Loss: 0.0011015275958925486\n",
      "Iter 321, Loss: 0.0013492549769580364\n",
      "Iter 322, Loss: 0.0012292914325371385\n",
      "Iter 323, Loss: 0.0011610420187935233\n",
      "Iter 324, Loss: 0.0012609096011146903\n",
      "Iter 325, Loss: 0.0010258076945319772\n",
      "Iter 326, Loss: 0.0012550713727250695\n",
      "Iter 327, Loss: 0.0014299738686531782\n",
      "Iter 328, Loss: 0.0007728652562946081\n",
      "Iter 329, Loss: 0.0015117381699383259\n",
      "Iter 330, Loss: 0.0011298313038423657\n",
      "Iter 331, Loss: 0.0014362522633746266\n",
      "Iter 332, Loss: 0.0007428184617310762\n",
      "Iter 333, Loss: 0.0016664417926222086\n",
      "Iter 334, Loss: 0.001180517254397273\n",
      "Iter 335, Loss: 0.0009687236743047833\n",
      "Iter 336, Loss: 0.0011440877569839358\n",
      "Iter 337, Loss: 0.0011225431226193905\n",
      "Iter 338, Loss: 0.0011620991863310337\n",
      "Iter 339, Loss: 0.001390044461004436\n",
      "Iter 340, Loss: 0.0006915178964845836\n",
      "Iter 341, Loss: 0.0009859873680397868\n",
      "Iter 342, Loss: 0.001095156534574926\n",
      "Iter 343, Loss: 0.000975991424638778\n",
      "Iter 344, Loss: 0.0013389239320531487\n",
      "Iter 345, Loss: 0.000984000158496201\n",
      "Iter 346, Loss: 0.0009437573608011007\n",
      "Iter 347, Loss: 0.0012480315053835511\n",
      "Iter 348, Loss: 0.0007816182333044708\n",
      "Iter 349, Loss: 0.0011684889905154705\n",
      "Iter 350, Loss: 0.001210348098538816\n",
      "Iter 351, Loss: 0.0009803932625800371\n",
      "Iter 352, Loss: 0.0011220640735700727\n",
      "Iter 353, Loss: 0.0009491642704233527\n",
      "Iter 354, Loss: 0.0010308042401447892\n",
      "Iter 355, Loss: 0.0009091012179851532\n",
      "Iter 356, Loss: 0.0010045635281130672\n",
      "Iter 357, Loss: 0.0009718582150526345\n",
      "Iter 358, Loss: 0.0010435791919007897\n",
      "Iter 359, Loss: 0.0008909779717214406\n",
      "Iter 360, Loss: 0.0009109977399930358\n",
      "Iter 361, Loss: 0.0008843595278449357\n",
      "Iter 362, Loss: 0.0012723326217383146\n",
      "Iter 363, Loss: 0.000845389615278691\n",
      "Iter 364, Loss: 0.0010556692723184824\n",
      "Iter 365, Loss: 0.0007454380975104868\n",
      "Iter 366, Loss: 0.0011780421482399106\n",
      "Iter 367, Loss: 0.0012360764667391777\n",
      "Iter 368, Loss: 0.0007533340831287205\n",
      "Iter 369, Loss: 0.0010458404431119561\n",
      "Iter 370, Loss: 0.0011629574000835419\n",
      "Iter 371, Loss: 0.0012981300242245197\n",
      "Iter 372, Loss: 0.0007847929955460131\n",
      "Iter 373, Loss: 0.0010487372055649757\n",
      "Iter 374, Loss: 0.0010969217400997877\n",
      "Iter 375, Loss: 0.0010468231048434973\n",
      "Iter 376, Loss: 0.0009574888390488923\n",
      "Iter 377, Loss: 0.0009831609204411507\n",
      "Iter 378, Loss: 0.0012760370736941695\n",
      "Iter 379, Loss: 0.0010093266610056162\n",
      "Iter 380, Loss: 0.0011604849714785814\n",
      "Iter 381, Loss: 0.0007723172893747687\n",
      "Iter 382, Loss: 0.00074175582267344\n",
      "Iter 383, Loss: 0.0008434012997895479\n",
      "Iter 384, Loss: 0.0008987460169009864\n",
      "Iter 385, Loss: 0.0008877232321538031\n",
      "Iter 386, Loss: 0.0008546551107428968\n",
      "Iter 387, Loss: 0.0008093030191957951\n",
      "Iter 388, Loss: 0.0007635340443812311\n",
      "Iter 389, Loss: 0.0008559401612728834\n",
      "Iter 390, Loss: 0.0010172022739425302\n",
      "Iter 391, Loss: 0.001024218276143074\n",
      "Iter 392, Loss: 0.0008587599731981754\n",
      "Iter 393, Loss: 0.000661234138533473\n",
      "Iter 394, Loss: 0.0008047503069974482\n",
      "Iter 395, Loss: 0.0007843978819437325\n",
      "Iter 396, Loss: 0.0010000739712268114\n",
      "Iter 397, Loss: 0.0008538268739357591\n",
      "Iter 398, Loss: 0.0008382211090065539\n",
      "Iter 399, Loss: 0.0008954796940088272\n",
      "Iter 400, Loss: 0.0008433699258603156\n",
      "Iter 401, Loss: 0.0009555077413097024\n",
      "Iter 402, Loss: 0.0009024426690302789\n",
      "Iter 403, Loss: 0.0008865869604051113\n",
      "Iter 404, Loss: 0.000860121101140976\n",
      "Iter 405, Loss: 0.000916517514269799\n",
      "Iter 406, Loss: 0.000840426015201956\n",
      "Iter 407, Loss: 0.0008740422199480236\n",
      "Iter 408, Loss: 0.0007550324662588537\n",
      "Iter 409, Loss: 0.0009475906263105571\n",
      "Iter 410, Loss: 0.0007059812778607011\n",
      "Iter 411, Loss: 0.0008164755417965353\n",
      "Iter 412, Loss: 0.0007997063803486526\n",
      "Iter 413, Loss: 0.0009949143277481198\n",
      "Iter 414, Loss: 0.0007371410611085594\n",
      "Iter 415, Loss: 0.0007670986815355718\n",
      "Iter 416, Loss: 0.0008405096014030278\n",
      "Iter 417, Loss: 0.0006058949511498213\n",
      "Iter 418, Loss: 0.0007532056188210845\n",
      "Iter 419, Loss: 0.0007659098482690752\n",
      "Iter 420, Loss: 0.0005639110459014773\n",
      "Iter 421, Loss: 0.0008347817929461598\n",
      "Iter 422, Loss: 0.0008801340009085834\n",
      "Iter 423, Loss: 0.0008738852338865399\n",
      "Iter 424, Loss: 0.0009805841837078333\n",
      "Iter 425, Loss: 0.0007772351964376867\n",
      "Iter 426, Loss: 0.0008439316297881305\n",
      "Iter 427, Loss: 0.0008023366099223495\n",
      "Iter 428, Loss: 0.0006919601582922041\n",
      "Iter 429, Loss: 0.0005944889271631837\n",
      "Iter 430, Loss: 0.0006772478809580207\n",
      "Iter 431, Loss: 0.0006627668044529855\n",
      "Iter 432, Loss: 0.000918432604521513\n",
      "Iter 433, Loss: 0.0007582883117720485\n",
      "Iter 434, Loss: 0.0008036868530325592\n",
      "Iter 435, Loss: 0.000606887333560735\n",
      "Iter 436, Loss: 0.0006069361115805805\n",
      "Iter 437, Loss: 0.0007290347712114453\n",
      "Iter 438, Loss: 0.0008857271750457585\n",
      "Iter 439, Loss: 0.0008119039703160524\n",
      "Iter 440, Loss: 0.0006464775069616735\n",
      "Iter 441, Loss: 0.0007077640620991588\n",
      "Iter 442, Loss: 0.0005825307453051209\n",
      "Iter 443, Loss: 0.0008270567050203681\n",
      "Iter 444, Loss: 0.0006490789237432182\n",
      "Iter 445, Loss: 0.000765730335842818\n",
      "Iter 446, Loss: 0.0008879632223397493\n",
      "Iter 447, Loss: 0.0008190384251065552\n",
      "Iter 448, Loss: 0.000610833871178329\n",
      "Iter 449, Loss: 0.0005347924306988716\n",
      "Iter 450, Loss: 0.0007844021311029792\n",
      "Iter 451, Loss: 0.0006868349155411124\n",
      "Iter 452, Loss: 0.0007685549790039659\n",
      "Iter 453, Loss: 0.000604561879299581\n",
      "Iter 454, Loss: 0.0007279454148374498\n",
      "Iter 455, Loss: 0.0005434317863546312\n",
      "Iter 456, Loss: 0.0008186998893506825\n",
      "Iter 457, Loss: 0.000771723163779825\n",
      "Iter 458, Loss: 0.0007684552692808211\n",
      "Iter 459, Loss: 0.00053995824418962\n",
      "Iter 460, Loss: 0.0006221271469257772\n",
      "Iter 461, Loss: 0.0006570132100023329\n",
      "Iter 462, Loss: 0.0006844183662906289\n",
      "Iter 463, Loss: 0.0005913670174777508\n",
      "Iter 464, Loss: 0.00043564150109887123\n",
      "Iter 465, Loss: 0.0007596428040415049\n",
      "Iter 466, Loss: 0.0006593288853764534\n",
      "Iter 467, Loss: 0.0006377885001711547\n",
      "Iter 468, Loss: 0.00044962854008190334\n",
      "Iter 469, Loss: 0.0005806185654364526\n",
      "Iter 470, Loss: 0.0005120261921547353\n",
      "Iter 471, Loss: 0.0007381376344710588\n",
      "Iter 472, Loss: 0.0008411777671426535\n",
      "Iter 473, Loss: 0.000521322712302208\n",
      "Iter 474, Loss: 0.0004609893658198416\n",
      "Iter 475, Loss: 0.0006339601241052151\n",
      "Iter 476, Loss: 0.0007167998701334\n",
      "Iter 477, Loss: 0.0005971517530269921\n",
      "Iter 478, Loss: 0.0005988622433505952\n",
      "Iter 479, Loss: 0.000825599126983434\n",
      "Iter 480, Loss: 0.000706794555298984\n",
      "Iter 481, Loss: 0.0006668356363661587\n",
      "Iter 482, Loss: 0.0006325584836304188\n",
      "Iter 483, Loss: 0.0006574415019713342\n",
      "Iter 484, Loss: 0.0004369303351268172\n",
      "Iter 485, Loss: 0.0006335803191177547\n",
      "Iter 486, Loss: 0.0007394057465717196\n",
      "Iter 487, Loss: 0.0006299815140664577\n",
      "Iter 488, Loss: 0.000382143392926082\n",
      "Iter 489, Loss: 0.0007042930228635669\n",
      "Iter 490, Loss: 0.0005472828634083271\n",
      "Iter 491, Loss: 0.00036089878994971514\n",
      "Iter 492, Loss: 0.0006620080093853176\n",
      "Iter 493, Loss: 0.00039971290971152484\n",
      "Iter 494, Loss: 0.0006595496088266373\n",
      "Iter 495, Loss: 0.000619012862443924\n",
      "Iter 496, Loss: 0.0005249890964478254\n",
      "Iter 497, Loss: 0.0006291170720942318\n",
      "Iter 498, Loss: 0.0005254686693660915\n",
      "Iter 499, Loss: 0.0005305851809680462\n",
      "Iter 500, Loss: 0.0006214050808921456\n",
      "Iter 501, Loss: 0.00039878347888588905\n",
      "Iter 502, Loss: 0.00041370801045559347\n",
      "Iter 503, Loss: 0.000468414364149794\n",
      "Iter 504, Loss: 0.0005927427555434406\n",
      "Iter 505, Loss: 0.0004092170565854758\n",
      "Iter 506, Loss: 0.0005461984546855092\n",
      "Iter 507, Loss: 0.0006891150260344148\n",
      "Iter 508, Loss: 0.0007228240719996393\n",
      "Iter 509, Loss: 0.0008556990651413798\n",
      "Iter 510, Loss: 0.0005452798213809729\n",
      "Iter 511, Loss: 0.00036139943404123187\n",
      "Iter 512, Loss: 0.0005835155607201159\n",
      "Iter 513, Loss: 0.0007059984491206706\n",
      "Iter 514, Loss: 0.0005799820646643639\n",
      "Iter 515, Loss: 0.0006601640488952398\n",
      "Iter 516, Loss: 0.00044952667667530477\n",
      "Iter 517, Loss: 0.0005811090813949704\n",
      "Iter 518, Loss: 0.0005460181855596602\n",
      "Iter 519, Loss: 0.0006025005131959915\n",
      "Iter 520, Loss: 0.0005021747783757746\n",
      "Iter 521, Loss: 0.0004971182788722217\n",
      "Iter 522, Loss: 0.0005792477750219405\n",
      "Iter 523, Loss: 0.000528795993886888\n",
      "Iter 524, Loss: 0.0004558349901344627\n",
      "Iter 525, Loss: 0.0006142894271761179\n",
      "Iter 526, Loss: 0.0006056111305952072\n",
      "Iter 527, Loss: 0.0005478628445416689\n",
      "Iter 528, Loss: 0.0006578592583537102\n",
      "Iter 529, Loss: 0.0005161631852388382\n",
      "Iter 530, Loss: 0.0004711129004135728\n",
      "Iter 531, Loss: 0.0003901016025338322\n",
      "Iter 532, Loss: 0.0006025181501172483\n",
      "Iter 533, Loss: 0.0005772561416961253\n",
      "Iter 534, Loss: 0.0005526531604118645\n",
      "Iter 535, Loss: 0.0004529506550170481\n",
      "Iter 536, Loss: 0.0005625804769806564\n",
      "Iter 537, Loss: 0.0005055528017692268\n",
      "Iter 538, Loss: 0.0005433291662484407\n",
      "Iter 539, Loss: 0.0005442592082545161\n",
      "Iter 540, Loss: 0.00047429371625185013\n",
      "Iter 541, Loss: 0.0007913055596873164\n",
      "Iter 542, Loss: 0.0004468982224352658\n",
      "Iter 543, Loss: 0.00041688786586746573\n",
      "Iter 544, Loss: 0.0005111358477734029\n",
      "Iter 545, Loss: 0.0004875152662862092\n",
      "Iter 546, Loss: 0.0005096492823213339\n",
      "Iter 547, Loss: 0.00033718362101353705\n",
      "Iter 548, Loss: 0.0004506286059040576\n",
      "Iter 549, Loss: 0.0005874492926523089\n",
      "Iter 550, Loss: 0.0005148343043401837\n",
      "Iter 551, Loss: 0.00033151358366012573\n",
      "Iter 552, Loss: 0.0007220694096758962\n",
      "Iter 553, Loss: 0.0004337598802521825\n",
      "Iter 554, Loss: 0.0005659016314893961\n",
      "Iter 555, Loss: 0.0006772937485948205\n",
      "Iter 556, Loss: 0.0005067524034529924\n",
      "Iter 557, Loss: 0.00037406233604997396\n",
      "Iter 558, Loss: 0.00042610723176039755\n",
      "Iter 559, Loss: 0.000364822888514027\n",
      "Iter 560, Loss: 0.0005357893533073366\n",
      "Iter 561, Loss: 0.00047775779967196286\n",
      "Iter 562, Loss: 0.0004055080353282392\n",
      "Iter 563, Loss: 0.00040717708179727197\n",
      "Iter 564, Loss: 0.0004530373262241483\n",
      "Iter 565, Loss: 0.0005329418345354497\n",
      "Iter 566, Loss: 0.000505647505633533\n",
      "Iter 567, Loss: 0.0005865109851583838\n",
      "Iter 568, Loss: 0.00044130723108537495\n",
      "Iter 569, Loss: 0.0005623804172500968\n",
      "Iter 570, Loss: 0.000513946870341897\n",
      "Iter 571, Loss: 0.00045489880722016096\n",
      "Iter 572, Loss: 0.0004033868608530611\n",
      "Iter 573, Loss: 0.0005683423369191587\n",
      "Iter 574, Loss: 0.00038198623224161565\n",
      "Iter 575, Loss: 0.0005078364629298449\n",
      "Iter 576, Loss: 0.0005128355114720762\n",
      "Iter 577, Loss: 0.00039508711779490113\n",
      "Iter 578, Loss: 0.0005189207149669528\n",
      "Iter 579, Loss: 0.0003844188177026808\n",
      "Iter 580, Loss: 0.0004789911035913974\n",
      "Iter 581, Loss: 0.0003877598501276225\n",
      "Iter 582, Loss: 0.00037909112870693207\n",
      "Iter 583, Loss: 0.00041366327786818147\n",
      "Iter 584, Loss: 0.00046329895849339664\n",
      "Iter 585, Loss: 0.0004892259021289647\n",
      "Iter 586, Loss: 0.0005989710334688425\n",
      "Iter 587, Loss: 0.00045852173934690654\n",
      "Iter 588, Loss: 0.0005179978907108307\n",
      "Iter 589, Loss: 0.00043902304605580866\n",
      "Iter 590, Loss: 0.0004034588928334415\n",
      "Iter 591, Loss: 0.0004535162297543138\n",
      "Iter 592, Loss: 0.00036812160396948457\n",
      "Iter 593, Loss: 0.0005024775164201856\n",
      "Iter 594, Loss: 0.00047027209075167775\n",
      "Iter 595, Loss: 0.0004957866622135043\n",
      "Iter 596, Loss: 0.0004087624547537416\n",
      "Iter 597, Loss: 0.00036959684803150594\n",
      "Iter 598, Loss: 0.0005052572232671082\n",
      "Iter 599, Loss: 0.0004822051851078868\n",
      "Iter 600, Loss: 0.000373031449271366\n",
      "Iter 601, Loss: 0.00040019600419327617\n",
      "Iter 602, Loss: 0.00048103046719916165\n",
      "Iter 603, Loss: 0.0003366615856066346\n",
      "Iter 604, Loss: 0.0004325103363953531\n",
      "Iter 605, Loss: 0.00034492704435251653\n",
      "Iter 606, Loss: 0.0004579275264404714\n",
      "Iter 607, Loss: 0.0003563898499123752\n",
      "Iter 608, Loss: 0.00053215870866552\n",
      "Iter 609, Loss: 0.0004230309568811208\n",
      "Iter 610, Loss: 0.0005557403201237321\n",
      "Iter 611, Loss: 0.0004364126652944833\n",
      "Iter 612, Loss: 0.0004648350295610726\n",
      "Iter 613, Loss: 0.00046180622302927077\n",
      "Iter 614, Loss: 0.00032798288157209754\n",
      "Iter 615, Loss: 0.00040123469079844654\n",
      "Iter 616, Loss: 0.000492857419885695\n",
      "Iter 617, Loss: 0.0003612882283050567\n",
      "Iter 618, Loss: 0.0004137032956350595\n",
      "Iter 619, Loss: 0.0003955168358515948\n",
      "Iter 620, Loss: 0.00041776426951400936\n",
      "Iter 621, Loss: 0.00047561750398017466\n",
      "Iter 622, Loss: 0.0003377511166036129\n",
      "Iter 623, Loss: 0.0004315855330787599\n",
      "Iter 624, Loss: 0.00028160083456896245\n",
      "Iter 625, Loss: 0.0004322798049543053\n",
      "Iter 626, Loss: 0.0003200034552719444\n",
      "Iter 627, Loss: 0.0004483113589230925\n",
      "Iter 628, Loss: 0.00039601651951670647\n",
      "Iter 629, Loss: 0.0005416127969510853\n",
      "Iter 630, Loss: 0.00039928455953486264\n",
      "Iter 631, Loss: 0.00048567671910859644\n",
      "Iter 632, Loss: 0.0004170358588453382\n",
      "Iter 633, Loss: 0.0004324762849137187\n",
      "Iter 634, Loss: 0.0003450122312642634\n",
      "Iter 635, Loss: 0.0004455560410860926\n",
      "Iter 636, Loss: 0.00038340972969308496\n",
      "Iter 637, Loss: 0.00048743622028268874\n",
      "Iter 638, Loss: 0.0004505280521698296\n",
      "Iter 639, Loss: 0.00038016404141671956\n",
      "Iter 640, Loss: 0.00039761688094586134\n",
      "Iter 641, Loss: 0.000421819364419207\n",
      "Iter 642, Loss: 0.000327275920426473\n",
      "Iter 643, Loss: 0.0003304154088255018\n",
      "Iter 644, Loss: 0.0004007347743026912\n",
      "Iter 645, Loss: 0.00036931189242750406\n",
      "Iter 646, Loss: 0.0004220024566166103\n",
      "Iter 647, Loss: 0.00037395773688331246\n",
      "Iter 648, Loss: 0.00045006992877461016\n",
      "Iter 649, Loss: 0.00029130911570973694\n",
      "Iter 650, Loss: 0.0004299031861592084\n",
      "Iter 651, Loss: 0.00034979163319803774\n",
      "Iter 652, Loss: 0.0003937818401027471\n",
      "Iter 653, Loss: 0.0004338683793321252\n",
      "Iter 654, Loss: 0.0004532448365353048\n",
      "Iter 655, Loss: 0.0004206499725114554\n",
      "Iter 656, Loss: 0.0003234579926356673\n",
      "Iter 657, Loss: 0.0002539816196076572\n",
      "Iter 658, Loss: 0.0004274955135770142\n",
      "Iter 659, Loss: 0.00035776689765043557\n",
      "Iter 660, Loss: 0.0004404214269015938\n",
      "Iter 661, Loss: 0.0003045249613933265\n",
      "Iter 662, Loss: 0.00032683537574484944\n",
      "Iter 663, Loss: 0.00033106270711869\n",
      "Iter 664, Loss: 0.0003331293410155922\n",
      "Iter 665, Loss: 0.000315178680466488\n",
      "Iter 666, Loss: 0.00034354516537860036\n",
      "Iter 667, Loss: 0.0003015296533703804\n",
      "Iter 668, Loss: 0.0003900115261785686\n",
      "Iter 669, Loss: 0.00035951033351011574\n",
      "Iter 670, Loss: 0.00035703054163604975\n",
      "Iter 671, Loss: 0.0003124395734630525\n",
      "Iter 672, Loss: 0.0003687830176204443\n",
      "Iter 673, Loss: 0.0003976559964939952\n",
      "Iter 674, Loss: 0.0003354023792780936\n",
      "Iter 675, Loss: 0.0003219129575882107\n",
      "Iter 676, Loss: 0.00025702198036015034\n",
      "Iter 677, Loss: 0.00026315735885873437\n",
      "Iter 678, Loss: 0.00033634944702498615\n",
      "Iter 679, Loss: 0.0004026450333185494\n",
      "Iter 680, Loss: 0.0003329529135953635\n",
      "Iter 681, Loss: 0.00041929175495170057\n",
      "Iter 682, Loss: 0.0002835967461578548\n",
      "Iter 683, Loss: 0.00030800202512182295\n",
      "Iter 684, Loss: 0.0003089381498284638\n",
      "Iter 685, Loss: 0.00041680963477119803\n",
      "Iter 686, Loss: 0.0003026883932761848\n",
      "Iter 687, Loss: 0.0003568102838471532\n",
      "Iter 688, Loss: 0.00030582083854824305\n",
      "Iter 689, Loss: 0.0002926142478827387\n",
      "Iter 690, Loss: 0.00023382817744277418\n",
      "Iter 691, Loss: 0.0003142975037917495\n",
      "Iter 692, Loss: 0.000378046534024179\n",
      "Iter 693, Loss: 0.0003369836776982993\n",
      "Iter 694, Loss: 0.00027302702073939145\n",
      "Iter 695, Loss: 0.0003294319030828774\n",
      "Iter 696, Loss: 0.0003889673971571028\n",
      "Iter 697, Loss: 0.0002594812249299139\n",
      "Iter 698, Loss: 0.0003044942277483642\n",
      "Iter 699, Loss: 0.000318953359965235\n",
      "Iter 700, Loss: 0.0002064879226963967\n",
      "Iter 701, Loss: 0.00025459512835368514\n",
      "Iter 702, Loss: 0.0003234411124140024\n",
      "Iter 703, Loss: 0.00038126599974930286\n",
      "Iter 704, Loss: 0.00030793508631177247\n",
      "Iter 705, Loss: 0.0003247439453843981\n",
      "Iter 706, Loss: 0.0002776689943857491\n",
      "Iter 707, Loss: 0.00036728137638419867\n",
      "Iter 708, Loss: 0.0003213864692952484\n",
      "Iter 709, Loss: 0.00030810770113021135\n",
      "Iter 710, Loss: 0.0003143534704577178\n",
      "Iter 711, Loss: 0.00044784348574467003\n",
      "Iter 712, Loss: 0.00027794347261078656\n",
      "Iter 713, Loss: 0.0003332985215820372\n",
      "Iter 714, Loss: 0.00025527633260935545\n",
      "Iter 715, Loss: 0.000397285824874416\n",
      "Iter 716, Loss: 0.0002571259974502027\n",
      "Iter 717, Loss: 0.00045612576650455594\n",
      "Iter 718, Loss: 0.000387674750527367\n",
      "Iter 719, Loss: 0.000296924845315516\n",
      "Iter 720, Loss: 0.000311441021040082\n",
      "Iter 721, Loss: 0.00021797261433675885\n",
      "Iter 722, Loss: 0.00033300957875326276\n",
      "Iter 723, Loss: 0.00033305902616120875\n",
      "Iter 724, Loss: 0.0002705843362491578\n",
      "Iter 725, Loss: 0.0003001442237291485\n",
      "Iter 726, Loss: 0.0003148775431327522\n",
      "Iter 727, Loss: 0.0002037831291090697\n",
      "Iter 728, Loss: 0.000356860866304487\n",
      "Iter 729, Loss: 0.00027921851142309606\n",
      "Iter 730, Loss: 0.0003247394342906773\n",
      "Iter 731, Loss: 0.0002812662278302014\n",
      "Iter 732, Loss: 0.0002661214384716004\n",
      "Iter 733, Loss: 0.0003351528139319271\n",
      "Iter 734, Loss: 0.0003583790676202625\n",
      "Iter 735, Loss: 0.00026059351512230933\n",
      "Iter 736, Loss: 0.00029415878816507757\n",
      "Iter 737, Loss: 0.00019155218615196645\n",
      "Iter 738, Loss: 0.00023839870118536055\n",
      "Iter 739, Loss: 0.00032791559351608157\n",
      "Iter 740, Loss: 0.000269038398982957\n",
      "Iter 741, Loss: 0.0003383528674021363\n",
      "Iter 742, Loss: 0.0002857765066437423\n",
      "Iter 743, Loss: 0.0002913938951678574\n",
      "Iter 744, Loss: 0.0002275340084452182\n",
      "Iter 745, Loss: 0.0003444157191552222\n",
      "Iter 746, Loss: 0.00030049370252527297\n",
      "Iter 747, Loss: 0.0002891208278015256\n",
      "Iter 748, Loss: 0.00037261791294440627\n",
      "Iter 749, Loss: 0.0002891626500058919\n",
      "Iter 750, Loss: 0.0002850279270205647\n",
      "Iter 751, Loss: 0.0002746619866229594\n",
      "Iter 752, Loss: 0.0003000509459525347\n",
      "Iter 753, Loss: 0.00032604188891127706\n",
      "Iter 754, Loss: 0.00038666240288875997\n",
      "Iter 755, Loss: 0.00029545609140768647\n",
      "Iter 756, Loss: 0.00025789253413677216\n",
      "Iter 757, Loss: 0.00038249106728471816\n",
      "Iter 758, Loss: 0.00025206623831763864\n",
      "Iter 759, Loss: 0.0002770511491689831\n",
      "Iter 760, Loss: 0.0002818965585902333\n",
      "Iter 761, Loss: 0.0002939426340162754\n",
      "Iter 762, Loss: 0.000199641814106144\n",
      "Iter 763, Loss: 0.00031052209669724107\n",
      "Iter 764, Loss: 0.00029385482775978744\n",
      "Iter 765, Loss: 0.0003119248431175947\n",
      "Iter 766, Loss: 0.00028889236273244023\n",
      "Iter 767, Loss: 0.00032499106600880623\n",
      "Iter 768, Loss: 0.0002552676305640489\n",
      "Iter 769, Loss: 0.00039278471376746893\n",
      "Iter 770, Loss: 0.0003293427871540189\n",
      "Iter 771, Loss: 0.0003609149716794491\n",
      "Iter 772, Loss: 0.00031007459620013833\n",
      "Iter 773, Loss: 0.0003112289123237133\n",
      "Iter 774, Loss: 0.00024114824191201478\n",
      "Iter 775, Loss: 0.00033845327561721206\n",
      "Iter 776, Loss: 0.0003291475586593151\n",
      "Iter 777, Loss: 0.0002577988198027015\n",
      "Iter 778, Loss: 0.0003634068416431546\n",
      "Iter 779, Loss: 0.00029068917501717806\n",
      "Iter 780, Loss: 0.0002535346138756722\n",
      "Iter 781, Loss: 0.0002505265292711556\n",
      "Iter 782, Loss: 0.00025405443739145994\n",
      "Iter 783, Loss: 0.0002555743558332324\n",
      "Iter 784, Loss: 0.0002673517738003284\n",
      "Iter 785, Loss: 0.00030403066193684936\n",
      "Iter 786, Loss: 0.00029769190587103367\n",
      "Iter 787, Loss: 0.0002645888598635793\n",
      "Iter 788, Loss: 0.0003237283381167799\n",
      "Iter 789, Loss: 0.0002731058339122683\n",
      "Iter 790, Loss: 0.00025554877356626093\n",
      "Iter 791, Loss: 0.00023602473083883524\n",
      "Iter 792, Loss: 0.00022130980505608022\n",
      "Iter 793, Loss: 0.0003182061482220888\n",
      "Iter 794, Loss: 0.00025873526465147734\n",
      "Iter 795, Loss: 0.0002621964958962053\n",
      "Iter 796, Loss: 0.0002302329958183691\n",
      "Iter 797, Loss: 0.00029844403616152704\n",
      "Iter 798, Loss: 0.00018122672918252647\n",
      "Iter 799, Loss: 0.0002138025447493419\n",
      "Iter 800, Loss: 0.0002839873777702451\n",
      "Iter 801, Loss: 0.00027057528495788574\n",
      "Iter 802, Loss: 0.00023537511879112571\n",
      "Iter 803, Loss: 0.00024035187379922718\n",
      "Iter 804, Loss: 0.00019929434347432107\n",
      "Iter 805, Loss: 0.0002219322050223127\n",
      "Iter 806, Loss: 0.000225384981604293\n",
      "Iter 807, Loss: 0.0002818747307173908\n",
      "Iter 808, Loss: 0.00022237480152398348\n",
      "Iter 809, Loss: 0.00031025195494294167\n",
      "Iter 810, Loss: 0.00021245490643195808\n",
      "Iter 811, Loss: 0.00026157626416534185\n",
      "Iter 812, Loss: 0.0002052615163847804\n",
      "Iter 813, Loss: 0.00017345357628073543\n",
      "Iter 814, Loss: 0.000280865904642269\n",
      "Iter 815, Loss: 0.0002953712537419051\n",
      "Iter 816, Loss: 0.00028252965421415865\n",
      "Iter 817, Loss: 0.00022762990556657314\n",
      "Iter 818, Loss: 0.00024183298228308558\n",
      "Iter 819, Loss: 0.00028243009001016617\n",
      "Iter 820, Loss: 0.00020155817037448287\n",
      "Iter 821, Loss: 0.00021608753013424575\n",
      "Iter 822, Loss: 0.00019449481624178588\n",
      "Iter 823, Loss: 0.00023285429051611573\n",
      "Iter 824, Loss: 0.00025425050989724696\n",
      "Iter 825, Loss: 0.00020877140923403203\n",
      "Iter 826, Loss: 0.00026430265279486775\n",
      "Iter 827, Loss: 0.00025460022152401507\n",
      "Iter 828, Loss: 0.00022714017541147768\n",
      "Iter 829, Loss: 0.0002277720777783543\n",
      "Iter 830, Loss: 0.00029604873270727694\n",
      "Iter 831, Loss: 0.000262475514318794\n",
      "Iter 832, Loss: 0.0002511728380341083\n",
      "Iter 833, Loss: 0.0002351037837797776\n",
      "Iter 834, Loss: 0.00022611444001086056\n",
      "Iter 835, Loss: 0.0002621151215862483\n",
      "Iter 836, Loss: 0.0002518311666790396\n",
      "Iter 837, Loss: 0.00021061304141767323\n",
      "Iter 838, Loss: 0.0002629760710988194\n",
      "Iter 839, Loss: 0.00026215429534204304\n",
      "Iter 840, Loss: 0.0002789897844195366\n",
      "Iter 841, Loss: 0.00023680960293859243\n",
      "Iter 842, Loss: 0.0002700383774936199\n",
      "Iter 843, Loss: 0.0002579262654762715\n",
      "Iter 844, Loss: 0.00020305122598074377\n",
      "Iter 845, Loss: 0.0002670653921086341\n",
      "Iter 846, Loss: 0.0003167887043673545\n",
      "Iter 847, Loss: 0.0002180121373385191\n",
      "Iter 848, Loss: 0.00024829793255776167\n",
      "Iter 849, Loss: 0.0002512335777282715\n",
      "Iter 850, Loss: 0.00020900077652186155\n",
      "Iter 851, Loss: 0.0002582838642410934\n",
      "Iter 852, Loss: 0.0002849782176781446\n",
      "Iter 853, Loss: 0.00032719969749450684\n",
      "Iter 854, Loss: 0.00022648200683761388\n",
      "Iter 855, Loss: 0.0002766550751402974\n",
      "Iter 856, Loss: 0.00023736707225907594\n",
      "Iter 857, Loss: 0.00023358430189546198\n",
      "Iter 858, Loss: 0.00026219323626719415\n",
      "Iter 859, Loss: 0.00027716936892829835\n",
      "Iter 860, Loss: 0.00022762968728784472\n",
      "Iter 861, Loss: 0.0002507171593606472\n",
      "Iter 862, Loss: 0.0002159768046112731\n",
      "Iter 863, Loss: 0.00025434393319301307\n",
      "Iter 864, Loss: 0.00018419616390019655\n",
      "Iter 865, Loss: 0.00019406068895477802\n",
      "Iter 866, Loss: 0.00020681595196947455\n",
      "Iter 867, Loss: 0.0002172455278923735\n",
      "Iter 868, Loss: 0.0002600626612547785\n",
      "Iter 869, Loss: 0.0002590014773886651\n",
      "Iter 870, Loss: 0.00023327008238993585\n",
      "Iter 871, Loss: 0.00026971890474669635\n",
      "Iter 872, Loss: 0.00021428284526336938\n",
      "Iter 873, Loss: 0.00022980534413363785\n",
      "Iter 874, Loss: 0.00018169070244766772\n",
      "Iter 875, Loss: 0.00019669911125674844\n",
      "Iter 876, Loss: 0.00024861140991561115\n",
      "Iter 877, Loss: 0.0002190658706240356\n",
      "Iter 878, Loss: 0.00018668775737751275\n",
      "Iter 879, Loss: 0.00022100181377027184\n",
      "Iter 880, Loss: 0.00021766179997939616\n",
      "Iter 881, Loss: 0.0002481757546775043\n",
      "Iter 882, Loss: 0.000220714311581105\n",
      "Iter 883, Loss: 0.0001952560996869579\n",
      "Iter 884, Loss: 0.0002159936702810228\n",
      "Iter 885, Loss: 0.0002160144067602232\n",
      "Iter 886, Loss: 0.00019868109666276723\n",
      "Iter 887, Loss: 0.00015740924573037773\n",
      "Iter 888, Loss: 0.00024332896282430738\n",
      "Iter 889, Loss: 0.00022246826847549528\n",
      "Iter 890, Loss: 0.00026181183056905866\n",
      "Iter 891, Loss: 0.00019438494928181171\n",
      "Iter 892, Loss: 0.00022359435388352722\n",
      "Iter 893, Loss: 0.0002322002110304311\n",
      "Iter 894, Loss: 0.0002545273455325514\n",
      "Iter 895, Loss: 0.00019879054161719978\n",
      "Iter 896, Loss: 0.00022331959917210042\n",
      "Iter 897, Loss: 0.00019764875469263643\n",
      "Iter 898, Loss: 0.00022983294911682606\n",
      "Iter 899, Loss: 0.00023016653722152114\n",
      "Iter 900, Loss: 0.00022870412794873118\n",
      "Iter 901, Loss: 0.0002315855963388458\n",
      "Iter 902, Loss: 0.0002218051813542843\n",
      "Iter 903, Loss: 0.00019497956964187324\n",
      "Iter 904, Loss: 0.0001902529620565474\n",
      "Iter 905, Loss: 0.0002356564364163205\n",
      "Iter 906, Loss: 0.00019809002697002143\n",
      "Iter 907, Loss: 0.00014146573084872216\n",
      "Iter 908, Loss: 0.00024227684480138123\n",
      "Iter 909, Loss: 0.00023933347256388515\n",
      "Iter 910, Loss: 0.0002086082095047459\n",
      "Iter 911, Loss: 0.0002017649239860475\n",
      "Iter 912, Loss: 0.00019976832845713943\n",
      "Iter 913, Loss: 0.00018174330762121826\n",
      "Iter 914, Loss: 0.00022392877144739032\n",
      "Iter 915, Loss: 0.00021296193881426007\n",
      "Iter 916, Loss: 0.00024098889844026417\n",
      "Iter 917, Loss: 0.00019443179189693183\n",
      "Iter 918, Loss: 0.00019575025362428278\n",
      "Iter 919, Loss: 0.00018915619875770062\n",
      "Iter 920, Loss: 0.0001937105116667226\n",
      "Iter 921, Loss: 0.0001911835715873167\n",
      "Iter 922, Loss: 0.00021327167632989585\n",
      "Iter 923, Loss: 0.00015943411563057452\n",
      "Iter 924, Loss: 0.00018779672973323613\n",
      "Iter 925, Loss: 0.00021733346511609852\n",
      "Iter 926, Loss: 0.00023069506278261542\n",
      "Iter 927, Loss: 0.00020160469284746796\n",
      "Iter 928, Loss: 0.00016409502131864429\n",
      "Iter 929, Loss: 0.0002145179605577141\n",
      "Iter 930, Loss: 0.00021811931219417602\n",
      "Iter 931, Loss: 0.00023975575459189713\n",
      "Iter 932, Loss: 0.00016062494250945747\n",
      "Iter 933, Loss: 0.00018223412916995585\n",
      "Iter 934, Loss: 0.0002084825828205794\n",
      "Iter 935, Loss: 0.00020261667668819427\n",
      "Iter 936, Loss: 0.0002050498005701229\n",
      "Iter 937, Loss: 0.00015535260899923742\n",
      "Iter 938, Loss: 0.00013142621901351959\n",
      "Iter 939, Loss: 0.0002047606831183657\n",
      "Iter 940, Loss: 0.00020012394816149026\n",
      "Iter 941, Loss: 0.0001528074499219656\n",
      "Iter 942, Loss: 0.0001852416608016938\n",
      "Iter 943, Loss: 0.00020186419715173542\n",
      "Iter 944, Loss: 0.00019038116442970932\n",
      "Iter 945, Loss: 0.00018533285765442997\n",
      "Iter 946, Loss: 0.00022177952632773668\n",
      "Iter 947, Loss: 0.0002350459835724905\n",
      "Iter 948, Loss: 0.0001775172568159178\n",
      "Iter 949, Loss: 0.00014143226144369692\n",
      "Iter 950, Loss: 0.00017101115372497588\n",
      "Iter 951, Loss: 0.00022006798826623708\n",
      "Iter 952, Loss: 0.00016524750390090048\n",
      "Iter 953, Loss: 0.00018803788407240063\n",
      "Iter 954, Loss: 0.0001779458107193932\n",
      "Iter 955, Loss: 0.00019379805598873645\n",
      "Iter 956, Loss: 0.00016312584921251982\n",
      "Iter 957, Loss: 0.00014429338625632226\n",
      "Iter 958, Loss: 0.00020992824283894151\n",
      "Iter 959, Loss: 0.00018649954290594906\n",
      "Iter 960, Loss: 0.00021818053210154176\n",
      "Iter 961, Loss: 0.00020982904243282974\n",
      "Iter 962, Loss: 0.00022295716917142272\n",
      "Iter 963, Loss: 0.0001640378322917968\n",
      "Iter 964, Loss: 0.00019366196647752076\n",
      "Iter 965, Loss: 0.0002041817642748356\n",
      "Iter 966, Loss: 0.00016764510655775666\n",
      "Iter 967, Loss: 0.00018767427536658943\n",
      "Iter 968, Loss: 0.00016223196871578693\n",
      "Iter 969, Loss: 0.00018672356964088976\n",
      "Iter 970, Loss: 0.0001861985365394503\n",
      "Iter 971, Loss: 0.0001571926986798644\n",
      "Iter 972, Loss: 0.00019318466365803033\n",
      "Iter 973, Loss: 0.0002016469807131216\n",
      "Iter 974, Loss: 0.0001961237721843645\n",
      "Iter 975, Loss: 0.0002286445815116167\n",
      "Iter 976, Loss: 0.0002030440664384514\n",
      "Iter 977, Loss: 0.00021301460219547153\n",
      "Iter 978, Loss: 0.00018049898790195584\n",
      "Iter 979, Loss: 0.00016506882093381137\n",
      "Iter 980, Loss: 0.00015543187328148633\n",
      "Iter 981, Loss: 0.00018991286924574524\n",
      "Iter 982, Loss: 0.0001730500371195376\n",
      "Iter 983, Loss: 0.00020405807299539447\n",
      "Iter 984, Loss: 0.000233877231949009\n",
      "Iter 985, Loss: 0.00016148301074281335\n",
      "Iter 986, Loss: 0.00019814482948277146\n",
      "Iter 987, Loss: 0.00013265157758723944\n",
      "Iter 988, Loss: 0.0002211594837717712\n",
      "Iter 989, Loss: 0.00016588714788667858\n",
      "Iter 990, Loss: 0.0001739195577101782\n",
      "Iter 991, Loss: 0.00015330457244999707\n",
      "Iter 992, Loss: 0.00018127576913684607\n",
      "Iter 993, Loss: 0.0001612181222299114\n",
      "Iter 994, Loss: 0.00013410668179858476\n",
      "Iter 995, Loss: 0.00017517627566121519\n",
      "Iter 996, Loss: 0.00021895015379413962\n",
      "Iter 997, Loss: 0.00019592448370531201\n",
      "Iter 998, Loss: 0.0001531823945697397\n",
      "Iter 999, Loss: 0.00015035393880680203\n"
     ]
    }
   ],
   "source": [
    "model = train(iter=32*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recog(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Recog, self).__init__()\n",
    "        \n",
    "        # 获取行和列的权重数据\n",
    "        row = model.row.data\n",
    "        col = model.col.data\n",
    "        \n",
    "        # 获取纵向的 best 列 (top-1) 和横向的 best 行 (top-3)\n",
    "        best_col = torch.topk(col, 1, -1).indices.flatten()  # 取列方向 top-1 索引\n",
    "        best_row = torch.topk(row, 3, -1).indices.flatten()  # 取行方向 top-3 索引\n",
    "\n",
    "        # 生成组合的 indices\n",
    "        self.indices = [(r.item(), best_col.item()) for r in best_row]\n",
    "        print(self.indices)\n",
    "        \n",
    "        # 分类器，输入维度 3，对应 top-3 的行信息，输出为 10 类\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        extracted_features = []\n",
    "\n",
    "            \n",
    "        # 遍历 self.indices 中的坐标对，提取对应位置的值\n",
    "        for (row_idx, col_idx) in self.indices:\n",
    "            feature_value = x[:, :, row_idx, col_idx]  # 提取特征值\n",
    "            extracted_features.append(feature_value)\n",
    "        # 将所有提取的特征堆叠为一个 tensor，形状为 [batch_size, 3]\n",
    "        extracted_features = torch.cat(extracted_features, dim=1)\n",
    "        \n",
    "        # 传入分类器进行分类\n",
    "        out = self.classifier(extracted_features)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine(model, batch_size=32, iters=1000, size=(28, 14)):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dataset = NumberDataset(size, iters=iters)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    model = Recog(model).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        images = images.float()\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Iter {i}, Loss: {loss.item()}')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(21, 9), (23, 9), (13, 9)]\n",
      "Iter 0, Loss: 114.80853271484375\n",
      "Iter 1, Loss: 166.4661102294922\n",
      "Iter 2, Loss: 133.0830841064453\n",
      "Iter 3, Loss: 88.10472869873047\n",
      "Iter 4, Loss: 124.2704849243164\n",
      "Iter 5, Loss: 98.24358367919922\n",
      "Iter 6, Loss: 61.2487907409668\n",
      "Iter 7, Loss: 60.45774459838867\n",
      "Iter 8, Loss: 40.07974624633789\n",
      "Iter 9, Loss: 39.68037414550781\n",
      "Iter 10, Loss: 44.85774230957031\n",
      "Iter 11, Loss: 54.256103515625\n",
      "Iter 12, Loss: 66.04409790039062\n",
      "Iter 13, Loss: 29.10091781616211\n",
      "Iter 14, Loss: 33.454246520996094\n",
      "Iter 15, Loss: 21.744857788085938\n",
      "Iter 16, Loss: 24.298070907592773\n",
      "Iter 17, Loss: 27.944316864013672\n",
      "Iter 18, Loss: 22.294322967529297\n",
      "Iter 19, Loss: 23.44327735900879\n",
      "Iter 20, Loss: 21.243778228759766\n",
      "Iter 21, Loss: 17.17523193359375\n",
      "Iter 22, Loss: 20.602893829345703\n",
      "Iter 23, Loss: 6.984119892120361\n",
      "Iter 24, Loss: 14.702832221984863\n",
      "Iter 25, Loss: 16.568063735961914\n",
      "Iter 26, Loss: 23.183303833007812\n",
      "Iter 27, Loss: 12.050646781921387\n",
      "Iter 28, Loss: 16.65751075744629\n",
      "Iter 29, Loss: 10.363382339477539\n",
      "Iter 30, Loss: 12.946809768676758\n",
      "Iter 31, Loss: 9.888340950012207\n",
      "Iter 32, Loss: 5.476527214050293\n",
      "Iter 33, Loss: 3.217310905456543\n",
      "Iter 34, Loss: 9.390172958374023\n",
      "Iter 35, Loss: 16.47527503967285\n",
      "Iter 36, Loss: 6.604204177856445\n",
      "Iter 37, Loss: 4.831868648529053\n",
      "Iter 38, Loss: 5.886927604675293\n",
      "Iter 39, Loss: 3.8591251373291016\n",
      "Iter 40, Loss: 3.297119140625\n",
      "Iter 41, Loss: 0.4246513247489929\n",
      "Iter 42, Loss: 2.8383231163024902\n",
      "Iter 43, Loss: 3.6119492053985596\n",
      "Iter 44, Loss: 4.251410484313965\n",
      "Iter 45, Loss: 5.497876167297363\n",
      "Iter 46, Loss: 2.714789867401123\n",
      "Iter 47, Loss: 4.9341301918029785\n",
      "Iter 48, Loss: 3.5811312198638916\n",
      "Iter 49, Loss: 2.7758548259735107\n",
      "Iter 50, Loss: 1.1143114566802979\n",
      "Iter 51, Loss: 2.6166281700134277\n",
      "Iter 52, Loss: 2.343177556991577\n",
      "Iter 53, Loss: 2.508815288543701\n",
      "Iter 54, Loss: 2.043517827987671\n",
      "Iter 55, Loss: 0.43666157126426697\n",
      "Iter 56, Loss: 1.5920617580413818\n",
      "Iter 57, Loss: 0.3975188732147217\n",
      "Iter 58, Loss: 0.9482694864273071\n",
      "Iter 59, Loss: 1.0276429653167725\n",
      "Iter 60, Loss: 0.918042004108429\n",
      "Iter 61, Loss: 0.2607634365558624\n",
      "Iter 62, Loss: 0.4194902181625366\n",
      "Iter 63, Loss: 1.0588092803955078\n",
      "Iter 64, Loss: 0.5705262422561646\n",
      "Iter 65, Loss: 1.6441633701324463\n",
      "Iter 66, Loss: 0.9507125616073608\n",
      "Iter 67, Loss: 0.16742995381355286\n",
      "Iter 68, Loss: 0.8260295987129211\n",
      "Iter 69, Loss: 2.645024061203003\n",
      "Iter 70, Loss: 2.6623048782348633\n",
      "Iter 71, Loss: 0.1920129358768463\n",
      "Iter 72, Loss: 0.30198076367378235\n",
      "Iter 73, Loss: 1.0377182960510254\n",
      "Iter 74, Loss: 1.3367619514465332\n",
      "Iter 75, Loss: 0.6142280101776123\n",
      "Iter 76, Loss: 0.050288621336221695\n",
      "Iter 77, Loss: 0.3653176426887512\n",
      "Iter 78, Loss: 0.5861263871192932\n",
      "Iter 79, Loss: 0.5168582797050476\n",
      "Iter 80, Loss: 0.06703773885965347\n",
      "Iter 81, Loss: 0.1830676943063736\n",
      "Iter 82, Loss: 0.06332220137119293\n",
      "Iter 83, Loss: 0.100064218044281\n",
      "Iter 84, Loss: 0.020832708105444908\n",
      "Iter 85, Loss: 0.35700753331184387\n",
      "Iter 86, Loss: 0.19686071574687958\n",
      "Iter 87, Loss: 0.0509311743080616\n",
      "Iter 88, Loss: 0.6856459379196167\n",
      "Iter 89, Loss: 0.01250924076884985\n",
      "Iter 90, Loss: 0.09255809336900711\n",
      "Iter 91, Loss: 0.16929380595684052\n",
      "Iter 92, Loss: 0.08096025884151459\n",
      "Iter 93, Loss: 0.0366554781794548\n",
      "Iter 94, Loss: 0.019606083631515503\n",
      "Iter 95, Loss: 0.014521350152790546\n",
      "Iter 96, Loss: 0.024363869801163673\n",
      "Iter 97, Loss: 0.03305121883749962\n",
      "Iter 98, Loss: 0.07215549796819687\n",
      "Iter 99, Loss: 0.022410046309232712\n",
      "Iter 100, Loss: 0.008458101190626621\n",
      "Iter 101, Loss: 0.01804717816412449\n",
      "Iter 102, Loss: 0.024077685549855232\n",
      "Iter 103, Loss: 0.025774477049708366\n",
      "Iter 104, Loss: 0.08678188174962997\n",
      "Iter 105, Loss: 0.018601126968860626\n",
      "Iter 106, Loss: 0.0150303915143013\n",
      "Iter 107, Loss: 0.03571809455752373\n",
      "Iter 108, Loss: 0.023535996675491333\n",
      "Iter 109, Loss: 0.45327839255332947\n",
      "Iter 110, Loss: 0.013029669411480427\n",
      "Iter 111, Loss: 0.23627829551696777\n",
      "Iter 112, Loss: 0.99703449010849\n",
      "Iter 113, Loss: 0.007089017424732447\n",
      "Iter 114, Loss: 0.007270581088960171\n",
      "Iter 115, Loss: 0.04673992097377777\n",
      "Iter 116, Loss: 0.496690958738327\n",
      "Iter 117, Loss: 0.013864517211914062\n",
      "Iter 118, Loss: 1.5067723989486694\n",
      "Iter 119, Loss: 1.3559422492980957\n",
      "Iter 120, Loss: 0.024927567690610886\n",
      "Iter 121, Loss: 2.3797638416290283\n",
      "Iter 122, Loss: 4.18566370010376\n",
      "Iter 123, Loss: 0.1998971551656723\n",
      "Iter 124, Loss: 0.6416499018669128\n",
      "Iter 125, Loss: 0.49407482147216797\n",
      "Iter 126, Loss: 0.79075026512146\n",
      "Iter 127, Loss: 0.012314270250499249\n",
      "Iter 128, Loss: 0.009143290109932423\n",
      "Iter 129, Loss: 1.5690456628799438\n",
      "Iter 130, Loss: 0.3375503420829773\n",
      "Iter 131, Loss: 0.6691825985908508\n",
      "Iter 132, Loss: 0.008098307996988297\n",
      "Iter 133, Loss: 0.2849790155887604\n",
      "Iter 134, Loss: 1.6701443195343018\n",
      "Iter 135, Loss: 2.097411870956421\n",
      "Iter 136, Loss: 0.01216916460543871\n",
      "Iter 137, Loss: 1.4655709266662598\n",
      "Iter 138, Loss: 0.8894891142845154\n",
      "Iter 139, Loss: 0.16410955786705017\n",
      "Iter 140, Loss: 1.3343377113342285\n",
      "Iter 141, Loss: 1.2519174814224243\n",
      "Iter 142, Loss: 0.6869166493415833\n",
      "Iter 143, Loss: 0.02271152101457119\n",
      "Iter 144, Loss: 0.7096987366676331\n",
      "Iter 145, Loss: 0.047641571611166\n",
      "Iter 146, Loss: 0.026913193985819817\n",
      "Iter 147, Loss: 0.31749600172042847\n",
      "Iter 148, Loss: 0.5500708818435669\n",
      "Iter 149, Loss: 0.040066760033369064\n",
      "Iter 150, Loss: 0.11335457116365433\n",
      "Iter 151, Loss: 0.32668352127075195\n",
      "Iter 152, Loss: 0.2995084226131439\n",
      "Iter 153, Loss: 0.14978225529193878\n",
      "Iter 154, Loss: 0.31132638454437256\n",
      "Iter 155, Loss: 0.3505895733833313\n",
      "Iter 156, Loss: 0.09779402613639832\n",
      "Iter 157, Loss: 0.03719056397676468\n",
      "Iter 158, Loss: 0.07326456159353256\n",
      "Iter 159, Loss: 0.12366902828216553\n",
      "Iter 160, Loss: 0.06512784957885742\n",
      "Iter 161, Loss: 0.05601375922560692\n",
      "Iter 162, Loss: 0.053834233433008194\n",
      "Iter 163, Loss: 0.029252244159579277\n",
      "Iter 164, Loss: 0.0359632708132267\n",
      "Iter 165, Loss: 0.01485576294362545\n",
      "Iter 166, Loss: 0.03503065183758736\n",
      "Iter 167, Loss: 0.029884040355682373\n",
      "Iter 168, Loss: 0.02346072904765606\n",
      "Iter 169, Loss: 0.01751650869846344\n",
      "Iter 170, Loss: 0.013555976562201977\n",
      "Iter 171, Loss: 0.004619910381734371\n",
      "Iter 172, Loss: 0.006794319953769445\n",
      "Iter 173, Loss: 0.008099053986370564\n",
      "Iter 174, Loss: 0.02001156099140644\n",
      "Iter 175, Loss: 0.007235754746943712\n",
      "Iter 176, Loss: 0.011596305295825005\n",
      "Iter 177, Loss: 0.0059288968332111835\n",
      "Iter 178, Loss: 0.009821533225476742\n",
      "Iter 179, Loss: 0.004389678593724966\n",
      "Iter 180, Loss: 0.005235053598880768\n",
      "Iter 181, Loss: 0.005272799637168646\n",
      "Iter 182, Loss: 0.00896479282528162\n",
      "Iter 183, Loss: 0.0018247256521135569\n",
      "Iter 184, Loss: 0.00826116930693388\n",
      "Iter 185, Loss: 0.0058795735239982605\n",
      "Iter 186, Loss: 0.004093768075108528\n",
      "Iter 187, Loss: 0.007564548868685961\n",
      "Iter 188, Loss: 0.0051003918051719666\n",
      "Iter 189, Loss: 0.00766485370695591\n",
      "Iter 190, Loss: 0.004125656560063362\n",
      "Iter 191, Loss: 0.004447132349014282\n",
      "Iter 192, Loss: 0.005434100516140461\n",
      "Iter 193, Loss: 0.004921568091958761\n",
      "Iter 194, Loss: 0.007332836743444204\n",
      "Iter 195, Loss: 0.007631790358573198\n",
      "Iter 196, Loss: 0.003075789660215378\n",
      "Iter 197, Loss: 0.005653431639075279\n",
      "Iter 198, Loss: 0.007551837246865034\n",
      "Iter 199, Loss: 0.008096580393612385\n",
      "Iter 200, Loss: 0.005710300989449024\n",
      "Iter 201, Loss: 0.001244008308276534\n",
      "Iter 202, Loss: 0.0043391031213104725\n",
      "Iter 203, Loss: 0.004509815014898777\n",
      "Iter 204, Loss: 0.004423542879521847\n",
      "Iter 205, Loss: 0.008140496909618378\n",
      "Iter 206, Loss: 0.005183178000152111\n",
      "Iter 207, Loss: 0.002574935555458069\n",
      "Iter 208, Loss: 0.003060063114389777\n",
      "Iter 209, Loss: 0.006224276497960091\n",
      "Iter 210, Loss: 0.006416226737201214\n",
      "Iter 211, Loss: 0.005755893886089325\n",
      "Iter 212, Loss: 0.0031581800431013107\n",
      "Iter 213, Loss: 0.004575049504637718\n",
      "Iter 214, Loss: 0.0018049718346446753\n",
      "Iter 215, Loss: 0.0027726483531296253\n",
      "Iter 216, Loss: 0.0027305155526846647\n",
      "Iter 217, Loss: 0.006291077937930822\n",
      "Iter 218, Loss: 0.0017734734574332833\n",
      "Iter 219, Loss: 0.004172482993453741\n",
      "Iter 220, Loss: 0.006960212253034115\n",
      "Iter 221, Loss: 0.0037179142236709595\n",
      "Iter 222, Loss: 0.004791325889527798\n",
      "Iter 223, Loss: 0.0016578284557908773\n",
      "Iter 224, Loss: 0.004085591062903404\n",
      "Iter 225, Loss: 0.0021257312037050724\n",
      "Iter 226, Loss: 0.005381401162594557\n",
      "Iter 227, Loss: 0.002652551280334592\n",
      "Iter 228, Loss: 0.0016177345532923937\n",
      "Iter 229, Loss: 0.005948500242084265\n",
      "Iter 230, Loss: 0.002080777660012245\n",
      "Iter 231, Loss: 0.0025525325909256935\n",
      "Iter 232, Loss: 0.0032836871687322855\n",
      "Iter 233, Loss: 0.0030156143475323915\n",
      "Iter 234, Loss: 0.001983456313610077\n",
      "Iter 235, Loss: 0.003458231221884489\n",
      "Iter 236, Loss: 0.0034283967688679695\n",
      "Iter 237, Loss: 0.0028943459037691355\n",
      "Iter 238, Loss: 0.002409211592748761\n",
      "Iter 239, Loss: 0.004729952663183212\n",
      "Iter 240, Loss: 0.003512410679832101\n",
      "Iter 241, Loss: 0.005340902134776115\n",
      "Iter 242, Loss: 0.002137842820957303\n",
      "Iter 243, Loss: 0.00408707931637764\n",
      "Iter 244, Loss: 0.0027379386592656374\n",
      "Iter 245, Loss: 0.00422648387029767\n",
      "Iter 246, Loss: 0.000654490664601326\n",
      "Iter 247, Loss: 0.0028930180706083775\n",
      "Iter 248, Loss: 0.0028451457619667053\n",
      "Iter 249, Loss: 0.002325710840523243\n",
      "Iter 250, Loss: 0.006024526432156563\n",
      "Iter 251, Loss: 0.002217506291344762\n",
      "Iter 252, Loss: 0.003175870282575488\n",
      "Iter 253, Loss: 0.003059352980926633\n",
      "Iter 254, Loss: 0.0023061472456902266\n",
      "Iter 255, Loss: 0.003084247699007392\n",
      "Iter 256, Loss: 0.0006027063936926425\n",
      "Iter 257, Loss: 0.0025909915566444397\n",
      "Iter 258, Loss: 0.002210983308032155\n",
      "Iter 259, Loss: 0.0030023311264812946\n",
      "Iter 260, Loss: 0.0029072046745568514\n",
      "Iter 261, Loss: 0.0025486256927251816\n",
      "Iter 262, Loss: 0.0041749258525669575\n",
      "Iter 263, Loss: 0.004282017238438129\n",
      "Iter 264, Loss: 0.004717222880572081\n",
      "Iter 265, Loss: 0.0018158868188038468\n",
      "Iter 266, Loss: 0.003358296351507306\n",
      "Iter 267, Loss: 0.0025362628512084484\n",
      "Iter 268, Loss: 0.0045289406552910805\n",
      "Iter 269, Loss: 0.0025870308745652437\n",
      "Iter 270, Loss: 0.0029700370505452156\n",
      "Iter 271, Loss: 0.0022051390260457993\n",
      "Iter 272, Loss: 0.0020141068380326033\n",
      "Iter 273, Loss: 0.0012556397123262286\n",
      "Iter 274, Loss: 0.0021773362532258034\n",
      "Iter 275, Loss: 0.001740321866236627\n",
      "Iter 276, Loss: 0.002403090475127101\n",
      "Iter 277, Loss: 0.0035958271473646164\n",
      "Iter 278, Loss: 0.0024001060519367456\n",
      "Iter 279, Loss: 0.001661736867390573\n",
      "Iter 280, Loss: 0.0021408090833574533\n",
      "Iter 281, Loss: 0.0016652082558721304\n",
      "Iter 282, Loss: 0.0022642388939857483\n",
      "Iter 283, Loss: 0.004156057257205248\n",
      "Iter 284, Loss: 0.0016821640310809016\n",
      "Iter 285, Loss: 0.0015995361609384418\n",
      "Iter 286, Loss: 0.002940733451396227\n",
      "Iter 287, Loss: 0.001210572780109942\n",
      "Iter 288, Loss: 0.0030741519294679165\n",
      "Iter 289, Loss: 0.0026930435560643673\n",
      "Iter 290, Loss: 0.0011106859892606735\n",
      "Iter 291, Loss: 0.0020114541985094547\n",
      "Iter 292, Loss: 0.0009305314160883427\n",
      "Iter 293, Loss: 0.002238966291770339\n",
      "Iter 294, Loss: 0.0015698785427957773\n",
      "Iter 295, Loss: 0.00222424091771245\n",
      "Iter 296, Loss: 0.0027858205139636993\n",
      "Iter 297, Loss: 0.0022251447662711143\n",
      "Iter 298, Loss: 0.0016405729111284018\n",
      "Iter 299, Loss: 0.00103712547570467\n",
      "Iter 300, Loss: 0.0021178610622882843\n",
      "Iter 301, Loss: 0.0009075799607671797\n",
      "Iter 302, Loss: 0.0015311038587242365\n",
      "Iter 303, Loss: 0.0023083644919097424\n",
      "Iter 304, Loss: 0.0012804546859115362\n",
      "Iter 305, Loss: 0.0021895775571465492\n",
      "Iter 306, Loss: 0.0015220021596178412\n",
      "Iter 307, Loss: 0.002768436213955283\n",
      "Iter 308, Loss: 0.0020939854439347982\n",
      "Iter 309, Loss: 0.002128066960722208\n",
      "Iter 310, Loss: 0.0016044091898947954\n",
      "Iter 311, Loss: 0.0032173434738069773\n",
      "Iter 312, Loss: 0.0020762404892593622\n",
      "Iter 313, Loss: 0.0013481631176546216\n",
      "Iter 314, Loss: 0.0007668836042284966\n",
      "Iter 315, Loss: 0.0028964614029973745\n",
      "Iter 316, Loss: 0.002186959143728018\n",
      "Iter 317, Loss: 0.0019240827532485127\n",
      "Iter 318, Loss: 0.0023786018136888742\n",
      "Iter 319, Loss: 0.0006500565796159208\n",
      "Iter 320, Loss: 0.0014289873652160168\n",
      "Iter 321, Loss: 0.0022826101630926132\n",
      "Iter 322, Loss: 0.0016735306708142161\n",
      "Iter 323, Loss: 0.0015766019932925701\n",
      "Iter 324, Loss: 0.002367939567193389\n",
      "Iter 325, Loss: 0.0022414959967136383\n",
      "Iter 326, Loss: 0.001412506913766265\n",
      "Iter 327, Loss: 0.001114110229536891\n",
      "Iter 328, Loss: 0.0009612526628188789\n",
      "Iter 329, Loss: 0.0015279467916116118\n",
      "Iter 330, Loss: 0.002160003874450922\n",
      "Iter 331, Loss: 0.0015249103307724\n",
      "Iter 332, Loss: 0.0018268418498337269\n",
      "Iter 333, Loss: 0.0009343721903860569\n",
      "Iter 334, Loss: 0.002290611620992422\n",
      "Iter 335, Loss: 0.00267618615180254\n",
      "Iter 336, Loss: 0.0015596632147207856\n",
      "Iter 337, Loss: 0.0018649572739377618\n",
      "Iter 338, Loss: 0.002297990955412388\n",
      "Iter 339, Loss: 0.0016665408620610833\n",
      "Iter 340, Loss: 0.0020032706670463085\n",
      "Iter 341, Loss: 0.0015204482479020953\n",
      "Iter 342, Loss: 0.0027737263590097427\n",
      "Iter 343, Loss: 0.0012104278430342674\n",
      "Iter 344, Loss: 0.0008301931084133685\n",
      "Iter 345, Loss: 0.00029113225173205137\n",
      "Iter 346, Loss: 0.0015859976410865784\n",
      "Iter 347, Loss: 0.0019576316699385643\n",
      "Iter 348, Loss: 0.002565947826951742\n",
      "Iter 349, Loss: 0.0021060246508568525\n",
      "Iter 350, Loss: 0.0011877227807417512\n",
      "Iter 351, Loss: 0.001255442388355732\n",
      "Iter 352, Loss: 0.002101329853758216\n",
      "Iter 353, Loss: 0.002285905182361603\n",
      "Iter 354, Loss: 0.0011528193717822433\n",
      "Iter 355, Loss: 0.001911557512357831\n",
      "Iter 356, Loss: 0.0017085722647607327\n",
      "Iter 357, Loss: 0.0024039435666054487\n",
      "Iter 358, Loss: 0.001975836232304573\n",
      "Iter 359, Loss: 0.0025871170219033957\n",
      "Iter 360, Loss: 0.0007944653043523431\n",
      "Iter 361, Loss: 0.0015378915704786777\n",
      "Iter 362, Loss: 0.002306682523339987\n",
      "Iter 363, Loss: 0.002542023779824376\n",
      "Iter 364, Loss: 0.0005881075048819184\n",
      "Iter 365, Loss: 0.001569333835504949\n",
      "Iter 366, Loss: 0.0012188309337943792\n",
      "Iter 367, Loss: 0.0017551733180880547\n",
      "Iter 368, Loss: 0.0010229425970464945\n",
      "Iter 369, Loss: 0.0020432579331099987\n",
      "Iter 370, Loss: 0.0015670314896851778\n",
      "Iter 371, Loss: 0.0010640433756634593\n",
      "Iter 372, Loss: 0.0018513500690460205\n",
      "Iter 373, Loss: 0.0007773087127134204\n",
      "Iter 374, Loss: 0.001328773214481771\n",
      "Iter 375, Loss: 0.0005713946302421391\n",
      "Iter 376, Loss: 0.0027961344458162785\n",
      "Iter 377, Loss: 0.0015638777986168861\n",
      "Iter 378, Loss: 0.0012703259708359838\n",
      "Iter 379, Loss: 0.002032361924648285\n",
      "Iter 380, Loss: 0.001501118647865951\n",
      "Iter 381, Loss: 0.0014328111428767443\n",
      "Iter 382, Loss: 0.0021718107163906097\n",
      "Iter 383, Loss: 0.001733357785269618\n",
      "Iter 384, Loss: 0.0024967631325125694\n",
      "Iter 385, Loss: 0.00222759204916656\n",
      "Iter 386, Loss: 0.0014649676159024239\n",
      "Iter 387, Loss: 0.0026768650859594345\n",
      "Iter 388, Loss: 0.0019362273160368204\n",
      "Iter 389, Loss: 0.001253687310963869\n",
      "Iter 390, Loss: 0.0014453670009970665\n",
      "Iter 391, Loss: 0.00120707624591887\n",
      "Iter 392, Loss: 0.0016684032743796706\n",
      "Iter 393, Loss: 0.00144245068076998\n",
      "Iter 394, Loss: 0.001937767956405878\n",
      "Iter 395, Loss: 0.001407395233400166\n",
      "Iter 396, Loss: 0.002129115629941225\n",
      "Iter 397, Loss: 0.0011996659450232983\n",
      "Iter 398, Loss: 0.0014309799298644066\n",
      "Iter 399, Loss: 0.00098693766631186\n",
      "Iter 400, Loss: 0.0012281043455004692\n",
      "Iter 401, Loss: 0.0011911764740943909\n",
      "Iter 402, Loss: 0.0014144695596769452\n",
      "Iter 403, Loss: 0.0018523512408137321\n",
      "Iter 404, Loss: 0.0016313140513375401\n",
      "Iter 405, Loss: 0.0007472776342183352\n",
      "Iter 406, Loss: 0.0009618289768695831\n",
      "Iter 407, Loss: 0.0020766945090144873\n",
      "Iter 408, Loss: 0.0025082528591156006\n",
      "Iter 409, Loss: 0.0020715519785881042\n",
      "Iter 410, Loss: 0.0011862515239045024\n",
      "Iter 411, Loss: 0.002724654972553253\n",
      "Iter 412, Loss: 0.0018126756185665727\n",
      "Iter 413, Loss: 0.0011461530812084675\n",
      "Iter 414, Loss: 0.0017973999492824078\n",
      "Iter 415, Loss: 0.0013680486008524895\n",
      "Iter 416, Loss: 0.00113820587284863\n",
      "Iter 417, Loss: 0.0011300531914457679\n",
      "Iter 418, Loss: 0.0020045919809490442\n",
      "Iter 419, Loss: 0.0009135401342064142\n",
      "Iter 420, Loss: 0.0018020450370386243\n",
      "Iter 421, Loss: 0.0013762300368398428\n",
      "Iter 422, Loss: 0.0017761820927262306\n",
      "Iter 423, Loss: 0.0018086815252900124\n",
      "Iter 424, Loss: 0.0013169067678973079\n",
      "Iter 425, Loss: 0.0026225000619888306\n",
      "Iter 426, Loss: 0.0011155091924592853\n",
      "Iter 427, Loss: 0.0013351485831663013\n",
      "Iter 428, Loss: 0.001732103293761611\n",
      "Iter 429, Loss: 0.0011029749875888228\n",
      "Iter 430, Loss: 0.0013036101590842009\n",
      "Iter 431, Loss: 0.0017101906705647707\n",
      "Iter 432, Loss: 0.0013367788633331656\n",
      "Iter 433, Loss: 0.000880163162946701\n",
      "Iter 434, Loss: 0.0023162623401731253\n",
      "Iter 435, Loss: 0.0008842850220389664\n",
      "Iter 436, Loss: 0.0012849560007452965\n",
      "Iter 437, Loss: 0.0010743967723101377\n",
      "Iter 438, Loss: 0.0010663606226444244\n",
      "Iter 439, Loss: 0.00108725402969867\n",
      "Iter 440, Loss: 0.0012975182617083192\n",
      "Iter 441, Loss: 0.0017763434443622828\n",
      "Iter 442, Loss: 0.0004466481041163206\n",
      "Iter 443, Loss: 0.0011225747875869274\n",
      "Iter 444, Loss: 0.0010656500235199928\n",
      "Iter 445, Loss: 0.0014794257003813982\n",
      "Iter 446, Loss: 0.0016523974481970072\n",
      "Iter 447, Loss: 0.0014907610602676868\n",
      "Iter 448, Loss: 0.0014434501063078642\n",
      "Iter 449, Loss: 0.0028872655238956213\n",
      "Iter 450, Loss: 0.0007895187009125948\n",
      "Iter 451, Loss: 0.0014929339522495866\n",
      "Iter 452, Loss: 0.0014987258473411202\n",
      "Iter 453, Loss: 0.0012248619459569454\n",
      "Iter 454, Loss: 0.0008498624665662646\n",
      "Iter 455, Loss: 0.0014604775933548808\n",
      "Iter 456, Loss: 0.0016595805063843727\n",
      "Iter 457, Loss: 0.001595602836459875\n",
      "Iter 458, Loss: 0.0010691590141505003\n",
      "Iter 459, Loss: 0.0018087977077811956\n",
      "Iter 460, Loss: 0.001490857801400125\n",
      "Iter 461, Loss: 0.0011866651475429535\n",
      "Iter 462, Loss: 0.0012596857268363237\n",
      "Iter 463, Loss: 0.001310323947109282\n",
      "Iter 464, Loss: 0.001429494470357895\n",
      "Iter 465, Loss: 0.000985237886197865\n",
      "Iter 466, Loss: 0.0012070263037458062\n",
      "Iter 467, Loss: 0.0020188959315419197\n",
      "Iter 468, Loss: 0.001807632390409708\n",
      "Iter 469, Loss: 0.0020226144697517157\n",
      "Iter 470, Loss: 0.0014250415842980146\n",
      "Iter 471, Loss: 0.0016308124177157879\n",
      "Iter 472, Loss: 0.0014249638188630342\n",
      "Iter 473, Loss: 0.001561418641358614\n",
      "Iter 474, Loss: 0.001751926727592945\n",
      "Iter 475, Loss: 0.0015969015657901764\n",
      "Iter 476, Loss: 0.001212252420373261\n",
      "Iter 477, Loss: 0.0010383903281763196\n",
      "Iter 478, Loss: 0.0008488886523991823\n",
      "Iter 479, Loss: 0.0013637664960697293\n",
      "Iter 480, Loss: 0.0011692865518853068\n",
      "Iter 481, Loss: 0.001033519278280437\n",
      "Iter 482, Loss: 0.0013461231719702482\n",
      "Iter 483, Loss: 0.0009557241573929787\n",
      "Iter 484, Loss: 0.0018073091050609946\n",
      "Iter 485, Loss: 0.0011061361292377114\n",
      "Iter 486, Loss: 0.0006599985063076019\n",
      "Iter 487, Loss: 0.000622378836851567\n",
      "Iter 488, Loss: 0.0006148360553197563\n",
      "Iter 489, Loss: 0.0006398013792932034\n",
      "Iter 490, Loss: 0.0010198274394497275\n",
      "Iter 491, Loss: 0.0018889436032623053\n",
      "Iter 492, Loss: 0.0011918222298845649\n",
      "Iter 493, Loss: 0.0011050303000956774\n",
      "Iter 494, Loss: 0.000899969891179353\n",
      "Iter 495, Loss: 0.0006459071883000433\n",
      "Iter 496, Loss: 0.0013344568433240056\n",
      "Iter 497, Loss: 0.0006612642901018262\n",
      "Iter 498, Loss: 0.001337338238954544\n",
      "Iter 499, Loss: 0.0008916610386222601\n"
     ]
    }
   ],
   "source": [
    "net = refine(model, iters=32*500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0.weight Parameter containing:\n",
      "tensor([[ 0.2946, -0.1655,  0.6585],\n",
      "        [-0.5076, -0.8356, -0.4289],\n",
      "        [ 0.0706,  0.5776,  0.4847],\n",
      "        [ 0.6253,  0.2506, -0.1518],\n",
      "        [ 0.0292,  0.3580,  0.6305],\n",
      "        [ 0.8279,  0.1487, -1.0714],\n",
      "        [ 0.4989,  0.7101, -0.8247],\n",
      "        [ 0.1666,  1.1126, -1.7727],\n",
      "        [ 0.4917,  0.2399,  0.2070],\n",
      "        [ 0.3406, -0.8976,  0.8716]], device='cuda:0', requires_grad=True)\n",
      "classifier.0.bias Parameter containing:\n",
      "tensor([-0.6053,  9.5437, -2.6593, -0.3391,  4.2295,  0.3279, -0.5293,  0.2820,\n",
      "        -1.1937, -0.6728], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def recog(image):\n",
    "    indices = ((21, 9), (23, 9), (13, 9))\n",
    "    \n",
    "    # 将权重和偏置转换为 NumPy 数组，便于矩阵运算\n",
    "    weights = np.array([\n",
    "        [ 0.2946, -0.1655,  0.6585],\n",
    "        [-0.5076, -0.8356, -0.4289],\n",
    "        [ 0.0706,  0.5776,  0.4847],\n",
    "        [ 0.6253,  0.2506, -0.1518],\n",
    "        [ 0.0292,  0.3580,  0.6305],\n",
    "        [ 0.8279,  0.1487, -1.0714],\n",
    "        [ 0.4989,  0.7101, -0.8247],\n",
    "        [ 0.1666,  1.1126, -1.7727],\n",
    "        [ 0.4917,  0.2399,  0.2070],\n",
    "        [ 0.3406, -0.8976,  0.8716]\n",
    "    ])\n",
    "    bias = np.array([-0.6053,  9.5437, -2.6593, -0.3391,  4.2295,  0.3279, -0.5293,  0.2820,\n",
    "                     -1.1937, -0.6728])\n",
    "    \n",
    "    # 将 Pillow 图像转换为 NumPy 数组（如果多次使用，最好在外部预先转换）\n",
    "    image_np = np.array(image)\n",
    "    \n",
    "    # 提取指定的 (row, col) 对应的特征值\n",
    "    extracted_features = np.array([image_np[row, col] for row, col in indices])\n",
    "    \n",
    "    # 使用矩阵运算来计算权重与特征的点积，并加上偏置\n",
    "    judge = np.dot(weights, extracted_features) + bias\n",
    "    \n",
    "    # 找到最大值的索引\n",
    "    return np.argmax(judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num):\n",
    "    image = number_to_image(num)\n",
    "    res = recog(image)\n",
    "    print(num, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "0 0\n",
      "1 1\n",
      "2 2\n",
      "3 3\n",
      "4 4\n",
      "5 5\n",
      "6 6\n",
      "7 7\n",
      "8 8\n",
      "9 9\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "for i in range(10):\n",
    "    test(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
